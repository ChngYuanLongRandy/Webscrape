{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogenerate a sample JD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate a sample JD, JR based on the information that we have here, we will finetune a GPT3 with the JD and JRs that we have.\n",
    "- Load up a dataset instance with all of the data\n",
    "- Write up the trainer instance\n",
    "- Train!\n",
    "- Save model\n",
    "- Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, AutoTokenizer, AutoModel, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = \"./train_jd.txt\"\n",
    "TEST_FILE_PATH = \"./test_jd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = data_pd_no_dup[data_pd_no_dup[\"job_description\"] != 'None']['job_description'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Develop targeted bespoke analytics models to help the venture extract value from, and monetise, data on our platforms, where such value extraction could include the likes of better client decisioning, pricing and opportunity identification..Research, design, implement and validate cutting-edge analytics and data visualization techniques to achieve targeted outcomes, such as bringing predicted outcomes closer to experience while ensuring consistency with the model ecosystem, while identifying opportunities for solutions to be leveraged across applications to broaden their scope of use and improve risk analysis..Collaborate with partner technology teams in setting up an effective model lifecycle platform on the cloud, with possible roles to play in cloud architecture and platform engineering..',\n",
       "       'Engage business users to discover how knowledge graph and/or NLP can enable operations transformation, such as to enhance operation effectiveness, raise productivity and support decision-making..Design, develop & deploy customised and scalable solutions as part of the end-to-end AI/ML pipeline for NLP..Design, develop & deploy customised and scalable knowledge graph solutions or products..Stay relevant in the latest technologies and trends in knowledge graph and NLP..Support company-wide initiatives to inspire and upskill colleagues..',\n",
       "       'Work with the Data Science team to transform data science prototypes into production models using best practices for model development, management and maintenance  .Research and implement appropriate ML algorithms and tools, select appropriate datasets, benchmark models, and perform statistical analysis and fine-tuning .Keep models up to date by incorporating continuous learning into system .Work closely with an entrepreneurial team of experienced researchers and software engineers to successfully ship software products and continue to grow our business .',\n",
       "       'Lead the continuous development of advanced analytics and AI/ML capabilities with a focus of NLP and Computer Vision..Implement scalable and robust AI/ML systems computing on large volumes of datasets using a variety of open source and proprietary Data Science technologies.End to end involvement in AI/ML projects from design, data processing, development, implementation, documentation, validation and optimization.Conduct research on AI/ML publications and be able to validate and implement according to local data and business needs.Work closely with stakeholders to ensure high standards of data governance during implementation.',\n",
       "       'Develop, test and maintain the data infrastructure, the data ingestion pipeline, data store and data processing on-premises and cloud.Design, build, deploy and manage end-to-end data pipelines for batch and stream processing that can adequately handle the needs of a rapidly growing data-driven organisation.Responsible for the building of scalable and reliable ETL processes for the ingestion and integration of large, structured and unstructured data from variety of data sources on-premises and on the cloud platforms.Implement and execute data security and data quality measurements to ensure compliance to data security and data governance policies.Implement analytic tools, analytic applications and user self-service portal for data scientists and business users\\u200b.'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = train_test_split(train_text, test_size=0.3, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_files(file_path, files):\n",
    "    with open(file_path, 'w', encoding='utf8') as writer:\n",
    "        for line in files:\n",
    "            writer.write(line)\n",
    "            writer.write(\"\\n\")\n",
    "\n",
    "write_text_files(TRAIN_FILE_PATH, train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finetuning of the model actually takes place in Google Colab because they have GPU there. \n",
    "\n",
    "Reference code: https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer= new_tokeniser,\n",
    "    file_path=TRAIN_FILE_PATH,\n",
    "    block_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\scrapper-env\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TextDataset(\n",
    "    tokenizer=new_tokeniser,\n",
    "    file_path=TEST_FILE_PATH,\n",
    "    block_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "CURR_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(CURR_DIR,'data')\n",
    "TRAIN_FILE_PATH = os.path.join(DATA_DIR,\"train_jd.txt\")\n",
    "TEST_FILE_PATH = os.path.join(DATA_DIR,\"test_jd.txt\")\n",
    "\n",
    "train_file_path = \"/content/drive/MyDrive/Articles.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = '/content/drive/MyDrive/result'\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 5.0\n",
    "save_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "train(\n",
    "    train_file_path=TRAIN_FILE_PATH,\n",
    "    model_name=model_name,\n",
    "    output_dir=CURR_DIR,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def generate_text(sequence, max_length):\n",
    "    model_path = \"/content/model\"\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = input() \n",
    "max_len = int(input()) \n",
    "generate_text(sequence, max_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it out at huggingface youself!\n",
    "https://huggingface.co/hashketh/gpt2-data-science-job-description?text=Job+Description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3247f7d4635bb288d9e06d3deacee818856115b2677ccfdf5a578edab993fe5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
